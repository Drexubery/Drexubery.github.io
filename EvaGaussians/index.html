<!DOCTYPE html>
<head>
    <meta charset="utf-8" />
    <title>EvaGaussians: Event Stream Assisted Gaussian Splatting from Blurry Images</title>
	<link rel="icon" type="image/x-icon" href="../assets/css/images/favicon.ico">
    <meta content="EvaGaussians: Event Stream Assisted Gaussian Splatting from Blurry Images" name="description" />
    <meta content="summary" name="twitter:card" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link href="static/css/template.css" rel="stylesheet" type="text/css" />
    <link href="static/css/my_style.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    
    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script type="text/javascript">
        WebFont.load({
            google: {
                families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Changa One:400,400italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500"]
            }
        });
    </script>
    <script type="text/javascript">
        ! function (o, c) {
            var n = c.documentElement,
                t = " w-mod-";
            n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch")
        }(window, document);
    </script>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script type="text/javascript" src="static/js/zoom.js"></script>
    <script type="text/javascript" src="static/js/video_comparison.js"></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MLDP9MKGC8"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-MLDP9MKGC8');
    </script>
    <style>
        hr {
        width: 81.5%; /* or any percentage or fixed value you want */
        text-align: center;
        margin-left: auto;
        margin-right: auto;
        }
        </style>
</head>

<body>
    <div class="section hero nerf-_v2">
        <div class="container-2 nerf_header_v2 w-container">
            <h1 class="nerf_title_v2">EvaGaussians: Event Stream Assisted Gaussian Splatting from Blurry Images</h1>
            <!-- <div class="nerf_subheader_v2">Anonymous NeurIPS Submission</div> -->
            <div class="nerf_subheader_v2">
                <div>
                    <a target="_blank" class="nerf_authors_v2 large_line_height">Wangbo Yu<span
                            class="text-span_nerf"></span></a><sup> *1,2</sup>,&nbsp;&nbsp;&nbsp;
                    <a target="_blank" class="nerf_authors_v2">Chaoran Feng<span
                        class="text-span_nerf"></span></a><sup> *1</sup>,&nbsp;&nbsp;&nbsp;
                        <a target="_blank" class="nerf_authors_v2">Jiye Tang<span
                            class="text-span_nerf"></span></a><sup> 3</sup>,&nbsp;&nbsp;&nbsp;
                                <a target="_blank" class="nerf_authors_v2">Xu Jia<span
                                    class="text-span_nerf"></span></a><sup> 3</sup>,&nbsp;&nbsp;&nbsp;
                                    <a target="_blank" class="nerf_authors_v2">Li Yuan<span
                                        class="text-span_nerf"></span></a><sup> 1,2</sup>,&nbsp;&nbsp;&nbsp;
                                            <a target="_blank" class="nerf_authors_v2">Yonghong Tian<span
                                                class="text-span_nerf"></span></a><sup> 1,2</sup>&nbsp;&nbsp;
                </div>
                <div>
                    <h1 class="nerf_affiliation_v2"><sup>1 </sup>Peking University</h1>,
                    <h1 class="nerf_affiliation_v2"><sup>2 </sup>Peng Cheng Laboratory</h1>,
                    <h1 class="nerf_affiliation_v2"><sup>3 </sup>Dalian University of Technology</h1>
                </div>
                <div class="external-link">
                    <a class="btn" href="" role="button" target="_blank">
                        <i class="ai ai-arxiv"></i> Arxiv </a>
                    <a class="btn" href="" role="button" target="_blank" disabled>
                        <i class="fa-brands fa-github"></i> Code (coming soon)</a>
                </div>
            </div>
        </div>

    </div>


    <div data-anchor="slide1" class="section nerf_section">
        <div class="w-container grey_container">
            <p class="grey-heading_nerf">Abstract</p>
            <p class="paragraph-3 nerf_text nerf_results_text">
                3D Gaussian Splatting (3D-GS) has demonstrated exceptional capabilities in 3D scene reconstruction and novel view synthesis. However, its training heavily depends on high-quality, sharp images and accurate camera poses. Fulfilling these requirements can be challenging in non-ideal real-world scenarios, where motion-blurred images are commonly encountered in high-speed moving cameras or low-light environments that require long exposure times.
To address these challenges, we introduce Event Stream Assisted Gaussian Splatting (<b>EvaGaussians</b>), a novel approach that integrates event streams captured by an event camera to assist in reconstructing high-quality 3D-GS from blurry images. Capitalizing on the high temporal resolution and dynamic range offered by the event camera, we leverage the event streams to explicitly model the formation process of motion-blurred images and guide the deblurring reconstruction of 3D-GS. By jointly optimizing the 3D-GS parameters and recovering camera motion trajectories during the exposure time, our method can robustly facilitate the acquisition of high-fidelity novel views with intricate texture details. We comprehensively evaluated our method and compared it with previous state-of-the-art deblurring rendering methods. Both qualitative and quantitative comparisons demonstrate that our method surpasses existing techniques in restoring fine details from blurry images and producing high-fidelity novel views.
                <br>
                <!-- <img  src="assets/images/overview.png"> -->
            </p>
        </div>
    </div>

    <div class="white_section_nerf  w-container">
        <p class="grey-heading_nerf">Method Overview</p>
        <!-- NVE pipe. -->
          <div class="content has-text-centered" style="text-align: center;">
            <img src="assets/images/pipeline.png"
                style="width: 100%"/>
          </div>
          <div class="content has-text-justified">
            <p>
                Overview of <b>EvaGaussians</b>. Our method seamlessly integrates the event streams captured by an event camera into the training of 3D-GS to robustly handle motion-blurred images.
    We adopt Event-based Double Integral (EDI) for blur modeling and preprocessing, yielding initial camera trajectories and sparse point cloud for 3D-GS training. By jointly optimizing the 3D-GS parameters and the camera motion trajectories using a blur reconstruction loss and an even reconstruction loss, our method facilitates high-quality 3D-GS reconstruction and novel view synthesis.
            </p>
          </div>
        <br/>
        <!--/ NVE pipeline. -->
    </div>

    <div class="white_section_nerf  w-container">
        <h2 class="grey-heading_nerf">Reconstruction Results</h2>
        <div class="content has-text-justified">
            <p>
                The left shows the blurry training images, and the right shows the deblurring rendering results of our method.
            </p>
          </div>
            <div>
                <video class="video" loop playsinline autoPlay muted src="assets/videos/city_block_visualization.mp4" ></video>
            </div>
            <div>
                <video class="video" loop playsinline autoPlay muted src="assets/videos/classroom_visualization.mp4"></video>
            </div>
            <div>
                <video class="video" loop playsinline autoPlay muted src="assets/videos/desert_visualization.mp4"></video>
            </div>
            <div>
                <video class="video" loop playsinline autoPlay muted src="assets/videos/forests_and_stones_visualization.mp4"></video>
            </div>
            <div>
                <video class="video" loop playsinline autoPlay muted src="assets/videos/lego_visualization.mp4"></video>
            </div>
            <div>
                <video class="video" loop playsinline autoPlay muted src="assets/videos/chair_visualization.mp4"></video>
            </div>
            <div>
                <video class="video" loop playsinline autoPlay muted src="assets/videos/hotdog_visualization.mp4"></video>
            </div>
            <div>
                <video class="video" loop playsinline autoPlay muted src="assets/videos/pokmer_visualization.mp4"></video>
            </div>
        </div>
    </div>
    <div class="white_section_nerf grey_container w-container">
        <h2 class="grey-heading_nerf">BibTeX</h2>
        <div class="bibtex">
            <pre><code>
        @article{yu2024eva,
            title={EvaGaussians: Event Stream Assisted Gaussian Splatting from Blurry Images},
            author={Yu, Wangbo and Feng, Chaoran and Tang, Jiye and Jia, Xu and Yuan, Li and Tian, Yonghong},
            journal={arXiv preprint},
            year={2024}
            }
        }</code></pre>
        </div>
        </div>
    </body>
    <footer>
        This project page is inspired by <a href="https://sweetdreamer3d.github.io/">SweetDreamer</a>.
    </footer>
    </html>
</body>

</html>

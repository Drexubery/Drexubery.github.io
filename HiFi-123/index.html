<!DOCTYPE html>
<head>
    <meta charset="utf-8" />
    <title>HiFi-123: Towards High-fidelity One Image to 3D Content Generation</title>
	<link rel="icon" type="image/x-icon" href="../assets/css/images/favicon.ico">
    <meta content="HiFi-123: Towards High-fidelity One Image to 3D Content Generation" name="description" />
    <meta content="summary" name="twitter:card" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link href="static/css/template.css" rel="stylesheet" type="text/css" />
    <link href="static/css/my_style.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    
    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script type="text/javascript">
        WebFont.load({
            google: {
                families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Changa One:400,400italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500"]
            }
        });
    </script>
    <script type="text/javascript">
        ! function (o, c) {
            var n = c.documentElement,
                t = " w-mod-";
            n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch")
        }(window, document);
    </script>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script type="text/javascript" src="static/js/zoom.js"></script>
    <script type="text/javascript" src="static/js/video_comparison.js"></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MLDP9MKGC8"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-MLDP9MKGC8');
    </script>
    <style>
        hr {
        width: 81.5%; /* or any percentage or fixed value you want */
        text-align: center;
        margin-left: auto;
        margin-right: auto;
        }
        </style>
    <style>
      .large_line_height {
        line-height: 2;  
      }
    </style>
</head>

<body>
    <div class="section hero nerf-_v2">
        <div class="container-2 nerf_header_v2 w-container">
            <h1 class="nerf_title_v2">HiFi-123: Towards High-fidelity One Image to 3D Content Generation</h1>
            <!-- <div class="nerf_subheader_v2">Anonymous ECCV Submission</div> -->
            <div class="nerf_subheader_v2">
                <div>
                    <a target="_blank" class="nerf_authors_v2 large_line_height">Wangbo Yu<span
                            class="text-span_nerf"></span></a><sup> 1,2</sup>,&nbsp;&nbsp;&nbsp;
                    <a target="_blank" class="nerf_authors_v2">Li Yuan<span
                        class="text-span_nerf"></span></a><sup> 1,2</sup>,&nbsp;&nbsp;&nbsp;
                        <a target="_blank" class="nerf_authors_v2">Yan-Pei Cao<span
                            class="text-span_nerf"></span></a><sup> 3</sup>,&nbsp;&nbsp;&nbsp;
                            <a target="_blank" class="nerf_authors_v2">Xiangjun Gao<span
                                class="text-span_nerf"></span></a><sup> 4</sup>,&nbsp;&nbsp;&nbsp;
                                <a target="_blank" class="nerf_authors_v2">Xiaoyu Li<span
                                    class="text-span_nerf"></span></a><sup> 3</sup>,&nbsp;&nbsp;&nbsp;
                                    <a target="_blank" class="nerf_authors_v2">Wenbo Hu<span
                                        class="text-span_nerf"></span></a><sup> 3</sup>,&nbsp;&nbsp;&nbsp;
                                        <a target="_blank" class="nerf_authors_v2">Long Quan<span
                                            class="text-span_nerf"></span></a><sup> 4</sup>,&nbsp;&nbsp;&nbsp;
                                        <a target="_blank" class="nerf_authors_v2">Ying Shan<span
                                            class="text-span_nerf"></span></a><sup> 3</sup>,&nbsp;&nbsp;&nbsp;
                                            <a target="_blank" class="nerf_authors_v2">Yonghong Tian<span
                                                class="text-span_nerf"></span></a><sup> 1,2</sup>,&nbsp;&nbsp;
                </div>
                <div>
                    <h1 class="nerf_affiliation_v2"><sup>1 </sup>Peking University</h1>,
                    <h1 class="nerf_affiliation_v2"><sup>2 </sup>Peng Cheng Laboratory</h1>,
                    <h1 class="nerf_affiliation_v2"><sup>3 </sup>Tencent AI Lab</h1>
                    <h1 class="nerf_affiliation_v2"><sup>3 </sup>HKUST</h1>
                </div>
                <div class="external-link">
                    <a class="btn" href="https://arxiv.org/abs/2310.06744" role="button" target="_blank">
                        <i class="ai ai-arxiv"></i> Arxiv </a>
                    <a class="btn" href="https://github.com/AILab-CVC/HiFi-123" role="button" target="_blank" disabled>
                        <i class="fa-brands fa-github"></i> Code </a>
                </div>
            </div>
        </div>

    </div>


    <div data-anchor="slide1" class="section nerf_section">
        <div class="w-container grey_container">
            <h2 class="grey-heading_nerf">Abstract</h2>
            <p class="paragraph-3 nerf_text nerf_results_text">
                Recent advances in diffusion models have enabled 3D generation from a single image. 
                However, current methods often produce suboptimal results for novel views, with blurred textures and deviations from the reference image, 
                limiting their practical applications. In this paper, we introduce HiFi-123, a method designed for high-fidelity and multi-view consistent 3D generation. 
                Our contributions are twofold: First, we propose a Reference-Guided Novel View Enhancement (RGNV) technique that significantly improves the fidelity 
                of diffusion-based zero-shot novel view synthesis methods. Second, capitalizing on the RGNV, we present a novel Reference-Guided State Distillation (RGSD) loss. 
                When incorporated into the optimization-based image-to-3D pipeline, our method significantly improves 3D generation quality, 
                achieving state-of-the-art performance. Comprehensive evaluations demonstrate the effectiveness of our approach over existing methods, 
                both qualitatively and quantitatively. Video comparisons are available on the supplementary project page. We will release our code to the public.
                <br>
                <!-- <img  src="assets/images/overview.png"> -->
            </p>
        </div>
    </div>

    <hr>
    <div class="white_section_nerf  w-container">
        <h2 class="grey-heading_nerf">Generated 3D Contents</h2>
            <div>
                <video class="video" loop playsinline autoPlay muted src="assets/videos/iron.mp4" ></video>
            </div>
            <div>
                <video class="video" loop playsinline autoPlay muted src="assets/videos/stw.mp4"></video>
            </div>
            <div>
                <video class="video" loop playsinline autoPlay muted src="assets/videos/war.mp4"></video>
            </div>
            <div>
            <video class="video" loop playsinline autoPlay muted src="assets/videos/cake.mp4"></video>
            </div>
            <div>
                <video class="video" loop playsinline autoPlay muted src="assets/videos/parrot.mp4"></video>
            </div>
            <div>
                <video class="video" loop playsinline autoPlay muted src="assets/videos/tower.mp4"></video>
            </div>
            <div>
                <video class="video" loop playsinline autoPlay muted src="assets/videos/rab.mp4"></video>
            </div>
        </div>
        <div class="grid-container-1">
            <a class="mybtn" href="clicks/dmtet-based-gallery_0.html" role="button">
             More Results (Click) </a>
        </div>
    </div>
    <hr>

    <div class="white_section_nerf  w-container">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="grey-heading_nerf">Comparisons</h2>
            <div class="videos">
              <video src="assets/videos/comparison.mp4" autoplay loop playsinline muted playbackRate="0.4" type="video/mp4"
              style="width: 100%; height: auto;"></video>
          </div>
      
          </div>
        </div>
      </div>
    <hr>
  <div class="white_section_nerf  w-container">
      <h2 class="grey-heading_nerf">Method Overview</h2>
      <!-- NVE pipe. -->
      <h3 class="title is-4">Reference-Guided Novel View Enhancement Pipeline</h3>
        <div class="content has-text-centered" style="text-align: center;">
          <img src="assets/images/RGNV.png"
              style="width: 100%"/>
        </div>
        <div class="content has-text-justified">
          <p>
              Illustration of the RGNV pipeline. It performs depth-based DDIM inversion and sampling on both the reference image and coarse novel view, and utilizes attention injection to transfer detail textures from the reference image to the coarse novel view.
          </p>
        </div>
      <br/>
      <!--/ NVE pipeline. -->

      <!-- 123. -->
      <h3 class="title is-4">Image-to-3D generation</h3>
      <div class="content has-text-centered" style="text-align: center;">
          <img src="assets/images/RGSD.png"
            style="width: 100%"/>
      </div>
      <div class="content has-text-justified">
          <p>
              We utilize two stages to generate high-fidelity 3D contents. In the coarse stage, we optimize an Instant-NGP representation using SDS loss, reference view reconstruction loss, depth loss, and normal loss. In the refine stage, we export DMTet representation and use our proposed RGSD loss to supervise training.
          </p>
        </div>
      <!--/ 123. -->
  </div>
  <div class="white_section_nerf grey_container w-container">
    <h2 class="grey-heading_nerf">BibTeX</h2>
    <div class="bibtex">
        <pre><code>
    @article{yu2023hifi,
        title={Hifi-123: Towards high-fidelity one image to 3d content generation},
        author={Yu, Wangbo and Yuan, Li and Cao, Yan-Pei and Gao, Xiangjun and Li, Xiaoyu 
            and Hu, Wenbo and Quan, Long and Shan, Ying and Tian, Yonghong},
        journal={arXiv preprint arXiv:2310.06744},
        year={2023}
        }
    }</code></pre>
    </div>
    </div>
</body>
<footer>
    This project page is inspired by <a href="https://sweetdreamer3d.github.io/">SweetDreamer</a>.
</footer>
</html>
